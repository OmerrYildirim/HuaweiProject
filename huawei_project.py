# -*- coding: utf-8 -*-
"""Huawei Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nvY9pC4nHxAtF3saqy_9oCFU_p5jlsNK
"""

import shutil
import os
import kagglehub

src_path = kagglehub.dataset_download("mohammedjaveed/loveda-dataset")
dst_path = "/content/loveda-dataset"

# Kopyala (veya taşı)
if not os.path.exists(dst_path):
    shutil.copytree(src_path, dst_path)

from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import os

class LoveDADataset(Dataset):
    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform
        self.mask_transform = mask_transform
        self.image_filenames = sorted(os.listdir(image_dir))  # ensure matching order
        self.mask_filenames = sorted(os.listdir(mask_dir))

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.image_filenames[idx])
        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])

        image = Image.open(img_path).convert("RGB")
        mask = Image.open(mask_path)  # do not convert to RGB

        if self.transform:
            image = self.transform(image)
        if self.mask_transform:
            mask = self.mask_transform(mask)

        mask = mask.squeeze(0)
        mask = mask.long()

        mask[mask == 7] = 0


        return image, mask

import torchvision.transforms as T

image_transform = T.Compose([
    T.Resize((256, 256)),
    T.ToTensor(),
])

mask_transform = T.Compose([
    T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),
    T.PILToTensor(),
])

"""
| Tür                  | Açıklama                                           | Avantajları                                | Dezavantajları                                | Kullanım Alanları                 |
| -------------------- | -------------------------------------------------- | ------------------------------------------ | --------------------------------------------- | --------------------------------- |
| **Nearest Neighbor** | En yakın pikselin değerini aynen alır.             | Çok hızlı, sınıf maskeleri için ideal.     | Kenarlar keskin olabilir, görüntü pürüzlü.    | Segmentasyon maskeleri, etiketler |
| **Bilinear**         | Komşu 4 pikselin ağırlıklı ortalamasını alır.      | Daha yumuşak sonuçlar, doğal görünüm.      | Maskelerde sınıf karışıklığı olabilir.        | Fotoğraflar, genel görüntüler     |
| **Bicubic**          | Komşu 16 pikselin karmaşık ağırlıklı ortalaması.   | Daha yumuşak, keskin ve kaliteli sonuçlar. | Daha yavaş, maskelerde hataya neden olabilir. | Fotoğraf büyütme, baskı işleri    |
| **Lanczos**          | Matematiksel olarak daha karmaşık, yüksek kaliteli | Çok keskin, kaliteli sonuç.                | Hesaplama maliyeti yüksek.                    | Profesyonel görüntü işleme        |
"""

from torch.utils.data import DataLoader, random_split

# Artık sabit path'in var:
image_dir = "/content/loveda-dataset/Train/Train/Urban/images_png"
mask_dir  = "/content/loveda-dataset/Train/Train/Urban/masks_png"

train_dataset = LoveDADataset(image_dir, mask_dir,
                               transform=image_transform,
                               mask_transform=mask_transform)

val_ratio = 0.2
val_size = int(len(train_dataset) * val_ratio)
train_size = len(train_dataset) - val_size

train_subset, val_subset = random_split(train_dataset, [train_size, val_size])

train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=8, shuffle=False)

"""8 batch size
3 Kanal sayısı (RGB için 3)
H Height
W Weight

"""

images, masks = next(iter(train_loader))
print(images.shape)  # e.g., torch.Size([8, 3, H, W])
print(masks.shape)   # e.g., torch.Size([8, 1, H, W])

class LoveDAClassNames:
    def __init__(self):
        self.classes = {
            0: "Background",
            1: "Building",
            2: "Road",
            3: "Water",
            4: "Barren",
            5: "Forest",
            6: "Agriculture",
        }

    def get_class_name(self, class_id):
        return self.classes.get(class_id, "Unknown")

    def print_all_classes(self):
        for class_id, class_name in self.classes.items():
            print(f"Class ID {class_id}: {class_name}")

# Kullanımı:
labels = LoveDAClassNames()
labels.print_all_classes()

print(labels.get_class_name(3))  # Output: Water

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as T
from PIL import Image
import os

"""UNET

"""

# === UNET_MODEL_START ===

class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    def forward(self, x):
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, n_classes):
        super().__init__()
        self.dconv_down1 = DoubleConv(3, 64)
        self.dconv_down2 = DoubleConv(64, 128)
        self.dconv_down3 = DoubleConv(128, 256)
        self.dconv_down4 = DoubleConv(256, 512)

        self.maxpool = nn.MaxPool2d(2)
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        self.dconv_up3 = DoubleConv(256 + 512, 256)
        self.dconv_up2 = DoubleConv(128 + 256, 128)
        self.dconv_up1 = DoubleConv(128 + 64, 64)

        self.conv_last = nn.Conv2d(64, n_classes, 1)

    def forward(self, x):
        # Encoder
        conv1 = self.dconv_down1(x)
        x = self.maxpool(conv1)

        conv2 = self.dconv_down2(x)
        x = self.maxpool(conv2)

        conv3 = self.dconv_down3(x)
        x = self.maxpool(conv3)

        x = self.dconv_down4(x)

        # Decoder
        x = self.upsample(x)
        x = torch.cat([x, conv3], dim=1)
        x = self.dconv_up3(x)

        x = self.upsample(x)
        x = torch.cat([x, conv2], dim=1)
        x = self.dconv_up2(x)

        x = self.upsample(x)
        x = torch.cat([x, conv1], dim=1)
        x = self.dconv_up1(x)

        out = self.conv_last(x)
        return out
# === UNET_MODEL_END ===

"""UNET_END"""

from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, jaccard_score
import numpy as np

def calculate_metrics(preds, targets, num_classes):
    preds = preds.cpu().numpy().flatten()
    targets = targets.cpu().numpy().flatten()

    # Precision, F1, Accuracy: macro average
    precision = precision_score(targets, preds, average='macro', zero_division=0)
    f1 = f1_score(targets, preds, average='macro', zero_division=0)
    acc = accuracy_score(targets, preds)
    iou = jaccard_score(targets, preds, average='macro', zero_division=0)

    return precision, f1, acc, iou

##YENİ
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, jaccard_score

def calculate_metrics(preds, targets, num_classes):
    preds_flat = preds.view(-1)
    targets_flat = targets.view(-1)

    precision = precision_score(targets_flat, preds_flat, average='macro', zero_division=0)
    recall = recall_score(targets_flat, preds_flat, average='macro', zero_division=0)
    f1 = f1_score(targets_flat, preds_flat, average='macro', zero_division=0)
    acc = accuracy_score(targets_flat, preds_flat)
    iou = jaccard_score(targets_flat, preds_flat, average='macro', zero_division=0)

    return precision, recall, f1, acc, iou

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = UNet(n_classes=7)
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
num_classes = 7

patience = 5
best_val_loss = float('inf')
epochs_no_improve = 0

num_epochs = 50

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    train_loss = running_loss / len(train_loader)

    # Validation aşaması
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for val_images, val_masks in val_loader:
            val_images = val_images.to(device)
            val_masks = val_masks.to(device)

            val_outputs = model(val_images)
            loss = criterion(val_outputs, val_masks)
            val_loss += loss.item()
    val_loss /= len(val_loader)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    # Early stopping ve best val loss kontrolü
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), "best_model.pth")  # En iyi modeli kaydet
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs.")
            break

# Eğitim sonrası model performansını değerlendirme
model.eval()
all_preds = []
all_targets = []
num_classes = 7

with torch.no_grad():
    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)

        outputs = model(images)  # [B, C, H, W]
        preds = torch.argmax(outputs, dim=1)  # [B, H, W]

        all_preds.append(preds.cpu())
        all_targets.append(masks.cpu())

# Tüm batch'leri birleştir
all_preds = torch.cat(all_preds, dim=0)
all_targets = torch.cat(all_targets, dim=0)

# Metrikleri hesapla
precision, recall, f1, acc, iou = calculate_metrics(all_preds, all_targets, num_classes)

print("\n=== Eğitim Sonu Genel Metrikler ===")
print(f"Accuracy :  {acc:.4f}")
print(f"F1 Score :  {f1:.4f}")
print(f"Precision:  {precision:.4f}")
print(f"Recall   :  {recall:.4f}")
print(f"IoU      :  {iou:.4f}")

import matplotlib.pyplot as plt
import numpy as np
import torch

def visualize_prediction(model, dataloader, device):
    model.eval()
    with torch.no_grad():
        images, masks = next(iter(dataloader))
        images = images.to(device)
        masks = masks.to(device)

        outputs = model(images)  # [B, n_classes, H, W]
        preds = torch.argmax(outputs, dim=1)  # [B, H, W]

        # İlk örneği alalım
        img = images[2].cpu()
        mask = masks[2].cpu()
        pred = preds[2].cpu()

        # Görüntüyü normalize edilmiş halinden orijinale döndür (denormalize)
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)
        img = img * std + mean
        img = img.clamp(0,1)

        # Maskeleri numpy array’e çevir
        mask = mask.numpy()
        pred = pred.numpy()

        # Renk paleti ve etiketler
        COLOR_MAP = {
          0: (255, 255, 255),   # Background
          1: (255, 0, 0),       # Building
          2: (255, 255, 0),     # Road
          3: (0, 0, 255),       # Water
          4: (159, 129, 183),   # Barren
          5: (0, 255, 0),       # Forest
          6: (255, 195, 128),   # Agricultural
          255: (0, 0, 0),       # IGNORE — siyah
        }



        labels = {
            0: "Background",
            1: "Building",
            2: "Road",
            3: "Water",
            4: "Barren",
            5: "Forest",
            6: "Agriculture",
            255: "IGNORE"
        }

        # Renkli maske oluşturucu
        def label_to_color(mask):
            h, w = mask.shape
            color_mask = np.zeros((h, w, 3), dtype=np.uint8)
            for class_id, color in COLOR_MAP.items():
               color_mask[mask == class_id] = color
            return color_mask

        color_mask = label_to_color(mask)
        color_pred = label_to_color(pred)

        # Görselleri göster
                # Görselleri göster
        fig, axs = plt.subplots(1, 3, figsize=(18, 6))

        axs[0].imshow(img.permute(1,2,0))
        axs[0].set_title("Original Image")
        axs[0].axis('off')

        axs[1].imshow(color_mask)
        axs[1].set_title("Ground Truth Mask")
        axs[1].axis('off')

        axs[2].imshow(color_pred)
        axs[2].set_title("Predicted Mask")
        axs[2].axis('off')

        # --- RENKLİ LEGEND BURADA EKLENİYOR ---
        from matplotlib.patches import Patch

        legend_elements = [
            Patch(facecolor=np.array(COLOR_MAP[class_id]) / 255.0, label=label)
            for class_id, label in labels.items()
        ]


        axs[2].legend(handles=legend_elements,
                      loc='lower right',
                      bbox_to_anchor=(1.5, 0),
                      title="Classes",
                      fontsize=10,
                      title_fontsize=12,
                      frameon=True)

        plt.tight_layout()
        plt.show()

visualize_prediction(model, train_loader, device)

"""FCN"""

import torchvision.models.segmentation as models

from torchvision.models.segmentation import fcn_resnet50

# Modeli yükle, pretrained=False (ImageNet değil)
fcn_model = fcn_resnet50(pretrained=False, num_classes=7)  # 7 sınıf
fcn_model = fcn_model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(fcn_model.parameters(), lr=1e-4)

num_epochs = 50
patience = 5
best_val_loss = float('inf')
epochs_no_improve = 0

model=fcn_model

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)

        optimizer.zero_grad()
        outputs = model(images)['out']
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    train_loss = running_loss / len(train_loader)

    # Validation aşaması
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for val_images, val_masks in val_loader:
            val_images = val_images.to(device)
            val_masks = val_masks.to(device)

            val_outputs = model(val_images)['out']
            loss = criterion(val_outputs, val_masks)
            val_loss += loss.item()

    val_loss /= len(val_loader)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    # Early stopping kontrolü
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
        # Modeli kaydetmek istersen burada yapabilirsin
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs.")
            break

# Eğitim sonrası model performansını değerlendirme
model.eval()
all_preds = []
all_targets = []
num_classes = 7

with torch.no_grad():
    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)

        outputs = model(images)["out"]  # [B, C, H, W]
        preds = torch.argmax(outputs, dim=1)  # [B, H, W]

        all_preds.append(preds.cpu())
        all_targets.append(masks.cpu())

# Tüm batch'leri birleştir
all_preds = torch.cat(all_preds, dim=0)
all_targets = torch.cat(all_targets, dim=0)

# Metrikleri hesapla
precision, recall, f1, acc, iou = calculate_metrics(all_preds, all_targets, num_classes)

print("\n=== Eğitim Sonu Genel Metrikler ===")
print(f"Accuracy :  {acc:.4f}")
print(f"F1 Score :  {f1:.4f}")
print(f"Precision:  {precision:.4f}")
print(f"Recall   :  {recall:.4f}")
print(f"IoU      :  {iou:.4f}")

def visualize_prediction(model, dataloader, device):
    model.eval()
    with torch.no_grad():
        images, masks = next(iter(dataloader))
        images = images.to(device)
        masks = masks.to(device)

        outputs = model(images)['out']  # FCN için yine ['out']
        preds = torch.argmax(outputs, dim=1)

         # İlk örneği alalım
        img = images[2].cpu()
        mask = masks[2].cpu()
        pred = preds[2].cpu()

        # Görüntüyü normalize edilmiş halinden orijinale döndür (denormalize)
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)
        img = img * std + mean
        img = img.clamp(0,1)

        # Maskeleri numpy array’e çevir
        mask = mask.numpy()
        pred = pred.numpy()

        # Renk paleti ve etiketler
        palette = {
            0: [255, 255, 255],     # Background - beyaz
            1: [128, 0, 0],         # Building - koyu kırmızı    Background
            2: [255, 255, 0],       # Road - sarı                Building olucak
            3: [0, 0, 128],         # Water - koyu mavi          Yol
            4: [128, 64, 128],      # Barren - morumsu
            5: [0, 128, 0],         # Forest - yeşil
            6: [189, 183, 107]      # Agriculture - bozkır rengi
        }

        labels = {
            0: "Background",
            1: "Building",
            2: "Road",
            3: "Water",
            4: "Barren",
            5: "Forest",
            6: "Agriculture"
        }

        # Renkli maske oluşturucu
        def label_to_color(mask):
            h, w = mask.shape
            color_mask = np.zeros((h, w, 3), dtype=np.uint8)
            for k, v in palette.items():
                color_mask[mask == k] = v
            return color_mask

        color_mask = label_to_color(mask)
        color_pred = label_to_color(pred)

        # Görselleri göster
                # Görselleri göster
        fig, axs = plt.subplots(1, 3, figsize=(18, 6))

        axs[0].imshow(img.permute(1,2,0))
        axs[0].set_title("Original Image")
        axs[0].axis('off')

        axs[1].imshow(color_mask)
        axs[1].set_title("Ground Truth Mask")
        axs[1].axis('off')

        axs[2].imshow(color_pred)
        axs[2].set_title("Predicted Mask")
        axs[2].axis('off')

        # --- RENKLİ LEGEND BURADA EKLENİYOR ---
        from matplotlib.patches import Patch

        legend_elements = [
            Patch(facecolor=np.array(color)/255.0, label=label)
            for label, color in zip(labels.values(), palette.values())
        ]

        axs[2].legend(handles=legend_elements,
                      loc='lower right',
                      bbox_to_anchor=(1.5, 0),
                      title="Classes",
                      fontsize=10,
                      title_fontsize=12,
                      frameon=True)

        plt.tight_layout()
        plt.show()

visualize_prediction(model, train_loader, device)

"""deeplab

"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.models.segmentation import deeplabv3_resnet50

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Modeli oluştur
model = deeplabv3_resnet50(pretrained=False, num_classes=7)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

num_epochs = 50
patience = 5
best_val_loss = float('inf')
epochs_no_improve = 0

for epoch in range(num_epochs):
    model.train()
    running_train_loss = 0.0

    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)

        optimizer.zero_grad()
        outputs = model(images)['out']
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item()

    avg_train_loss = running_train_loss / len(train_loader)

    model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for val_images, val_masks in val_loader:
            val_images = val_images.to(device)
            val_masks = val_masks.to(device)
            val_outputs = model(val_images)['out']
            val_loss = criterion(val_outputs, val_masks)
            running_val_loss += val_loss.item()

    avg_val_loss = running_val_loss / len(val_loader)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

    # Early stopping kontrolü
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), "best_deeplab_model.pth")  # En iyi modeli kaydet
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= patience:
            print("Early stopping triggered")
            break

# Eğitim sonrası model performansını değerlendirme
model.eval()
all_preds = []
all_targets = []
num_classes = 7

with torch.no_grad():
    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)

        outputs = model(images)["out"]  # [B, C, H, W]
        preds = torch.argmax(outputs, dim=1)  # [B, H, W]

        all_preds.append(preds.cpu())
        all_targets.append(masks.cpu())

# Tüm batch'leri birleştir
all_preds = torch.cat(all_preds, dim=0)
all_targets = torch.cat(all_targets, dim=0)

# Metrikleri hesapla
precision, recall, f1, acc, iou = calculate_metrics(all_preds, all_targets, num_classes)

print("\n=== Eğitim Sonu Genel Metrikler ===")
print(f"Accuracy :  {acc:.4f}")
print(f"F1 Score :  {f1:.4f}")
print(f"Precision:  {precision:.4f}")
print(f"Recall   :  {recall:.4f}")
print(f"IoU      :  {iou:.4f}")

def visualize_prediction(model, dataloader, device):
    model.eval()
    with torch.no_grad():
        images, masks = next(iter(dataloader))
        images = images.to(device)
        masks = masks.to(device)

        outputs = model(images)['out']  # FPN için yine ['out']
        preds = torch.argmax(outputs, dim=1)

         # İlk örneği alalım
        img = images[2].cpu()
        mask = masks[2].cpu()
        pred = preds[2].cpu()

        # Görüntüyü normalize edilmiş halinden orijinale döndür (denormalize)
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)
        img = img * std + mean
        img = img.clamp(0,1)

        # Maskeleri numpy array’e çevir
        mask = mask.numpy()
        pred = pred.numpy()

        # Renk paleti ve etiketler
        palette = {
            0: [255, 255, 255],     # Background - beyaz
            1: [128, 0, 0],         # Building - koyu kırmızı    Background
            2: [255, 255, 0],       # Road - sarı                Building olucak
            3: [0, 0, 128],         # Water - koyu mavi          Yol
            4: [128, 64, 128],      # Barren - morumsu
            5: [0, 128, 0],         # Forest - yeşil
            6: [189, 183, 107]      # Agriculture - bozkır rengi
        }

        labels = {
            0: "Background",
            1: "Building",
            2: "Road",
            3: "Water",
            4: "Barren",
            5: "Forest",
            6: "Agriculture"
        }

        # Renkli maske oluşturucu
        def label_to_color(mask):
            h, w = mask.shape
            color_mask = np.zeros((h, w, 3), dtype=np.uint8)
            for k, v in palette.items():
                color_mask[mask == k] = v
            return color_mask

        color_mask = label_to_color(mask)
        color_pred = label_to_color(pred)

        # Görselleri göster
                # Görselleri göster
        fig, axs = plt.subplots(1, 3, figsize=(18, 6))

        axs[0].imshow(img.permute(1,2,0))
        axs[0].set_title("Original Image")
        axs[0].axis('off')

        axs[1].imshow(color_mask)
        axs[1].set_title("Ground Truth Mask")
        axs[1].axis('off')

        axs[2].imshow(color_pred)
        axs[2].set_title("Predicted Mask")
        axs[2].axis('off')

        # --- RENKLİ LEGEND BURADA EKLENİYOR ---
        from matplotlib.patches import Patch

        legend_elements = [
            Patch(facecolor=np.array(color)/255.0, label=label)
            for label, color in zip(labels.values(), palette.values())
        ]

        axs[2].legend(handles=legend_elements,
                      loc='lower right',
                      bbox_to_anchor=(1.5, 0),
                      title="Classes",
                      fontsize=10,
                      title_fontsize=12,
                      frameon=True)

        plt.tight_layout()
        plt.show()

visualize_prediction(model, train_loader, device)

!pip install segmentation_models_pytorch

import segmentation_models_pytorch as smp

"""


PSPNet
"""

import torch
import torch.nn as nn
import torch.optim as optim

# PSPNet modelini uygun şekilde içe aktar
# Örneğin:
# from your_model_file import PSPNet  # kendi model tanımına göre düzenle

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# PSPNet modelini oluştur

model = smp.PSPNet(
    encoder_name="resnet34",      # backbone (dilersen resnet50, efficientnet, vb.)
    encoder_weights="imagenet",   # önceden eğitilmiş ağırlıklar
    in_channels=3,                # Giriş kanal sayısı (RGB için 3)
    classes=7                     # Çıkış sınıf sayısı
)  # num_classes çıktıya göre ayarlanmalı
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

patience = 5
best_val_loss = float('inf')
epochs_no_improve = 0

num_epochs = 50

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)

        optimizer.zero_grad()
        outputs = model(images)

        # Eğer output shape [B, C, H, W] ve mask shape [B, H, W] ise bu doğrudur
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    train_loss = running_loss / len(train_loader)

    # Validation aşaması
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for val_images, val_masks in val_loader:
            val_images = val_images.to(device)
            val_masks = val_masks.to(device)

            val_outputs = model(val_images)
            loss = criterion(val_outputs, val_masks)
            val_loss += loss.item()
    val_loss /= len(val_loader)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    # Early stopping ve best val loss kontrolü
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), "best_pspnet_model.pth")  # En iyi modeli kaydet
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs.")
            break

# Eğitim sonrası model performansını değerlendirme
model.eval()
all_preds = []
all_targets = []
num_classes = 7

with torch.no_grad():
    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)

        outputs = model(images)  # [B, C, H, W]
        preds = torch.argmax(outputs, dim=1)  # [B, H, W]

        all_preds.append(preds.cpu())
        all_targets.append(masks.cpu())

# Tüm batch'leri birleştir
all_preds = torch.cat(all_preds, dim=0)
all_targets = torch.cat(all_targets, dim=0)

# Metrikleri hesapla
precision, recall, f1, acc, iou = calculate_metrics(all_preds, all_targets, num_classes)

print("\n=== Eğitim Sonu Genel Metrikler ===")
print(f"Accuracy :  {acc:.4f}")
print(f"F1 Score :  {f1:.4f}")
print(f"Precision:  {precision:.4f}")
print(f"Recall   :  {recall:.4f}")
print(f"IoU      :  {iou:.4f}")

def visualize_prediction(model, dataloader, device):
    model.eval()
    with torch.no_grad():
        images, masks = next(iter(dataloader))
        images = images.to(device)
        masks = masks.to(device)

        outputs = model(images)
        preds = torch.argmax(outputs, dim=1)

         # İlk örneği alalım
        img = images[2].cpu()
        mask = masks[2].cpu()
        pred = preds[2].cpu()

        # Görüntüyü normalize edilmiş halinden orijinale döndür (denormalize)
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)
        img = img * std + mean
        img = img.clamp(0,1)

        # Maskeleri numpy array’e çevir
        mask = mask.numpy()
        pred = pred.numpy()

        # Renk paleti ve etiketler
        palette = {
            0: [255, 255, 255],     # Background - beyaz
            1: [128, 0, 0],         # Building - koyu kırmızı    Background
            2: [255, 255, 0],       # Road - sarı                Building olucak
            3: [0, 0, 128],         # Water - koyu mavi          Yol
            4: [128, 64, 128],      # Barren - morumsu
            5: [0, 128, 0],         # Forest - yeşil
            6: [189, 183, 107]      # Agriculture - bozkır rengi
        }

        labels = {
            0: "Background",
            1: "Building",
            2: "Road",
            3: "Water",
            4: "Barren",
            5: "Forest",
            6: "Agriculture"
        }

        # Renkli maske oluşturucu
        def label_to_color(mask):
            h, w = mask.shape
            color_mask = np.zeros((h, w, 3), dtype=np.uint8)
            for k, v in palette.items():
                color_mask[mask == k] = v
            return color_mask

        color_mask = label_to_color(mask)
        color_pred = label_to_color(pred)

        # Görselleri göster
                # Görselleri göster
        fig, axs = plt.subplots(1, 3, figsize=(18, 6))

        axs[0].imshow(img.permute(1,2,0))
        axs[0].set_title("Original Image")
        axs[0].axis('off')

        axs[1].imshow(color_mask)
        axs[1].set_title("Ground Truth Mask")
        axs[1].axis('off')

        axs[2].imshow(color_pred)
        axs[2].set_title("Predicted Mask")
        axs[2].axis('off')

        # --- RENKLİ LEGEND BURADA EKLENİYOR ---
        from matplotlib.patches import Patch

        legend_elements = [
            Patch(facecolor=np.array(color)/255.0, label=label)
            for label, color in zip(labels.values(), palette.values())
        ]

        axs[2].legend(handles=legend_elements,
                      loc='lower right',
                      bbox_to_anchor=(1.5, 0),
                      title="Classes",
                      fontsize=10,
                      title_fontsize=12,
                      frameon=True)

        plt.tight_layout()
        plt.show()

visualize_prediction(model, train_loader, device)

;